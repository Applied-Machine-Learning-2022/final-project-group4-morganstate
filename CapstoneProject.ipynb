{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapstoneProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "XxZaDpExfg2k",
        "outputId": "5d158cc6-c422-4ea0-ff90-84d76b5e54b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "11314\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3a7d625d75ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mnewsgroups_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'footers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quotes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# # newsgroups_train.sample(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
          ]
        }
      ],
      "source": [
        "#IGNORE THIS CODE, THE CODE FOR PART 1 & 2 IS AFTER THIS CELL\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "#pprint(list(newsgroups_train.target_names))\n",
        "\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "# newsgroups_train['data']\n",
        "\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'),categories=categories)\n",
        "# # newsgroups_train.sample(10)\n",
        "\n",
        "\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# #df1 = pd.DataFrame(columns=['sci.electronics', 'talk.religion.misc', 'rec.motorcycles'])\n",
        "# #df1.info()\n",
        "\n",
        "# frequency = ['sci.electronics', 'talk.religion.misc', 'rec.motorcycles']\n",
        "# newsgroups_train  = fetch_20newsgroups(subset='train', categories=frequency)\n",
        "# list(newsgroups_train.target_names)\n",
        "\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "# vectors.shape\n",
        "#vectors.nnz / float(vectors.shape[0])\n",
        "\n",
        "# data = frequency\n",
        "# vectorizer = CountVectorizer()\n",
        "# vectorizer.fit(frequency)\n",
        "\n",
        "# data_vec = vectorizer.transform(frequency)\n",
        "\n",
        "# print(data_vec)\n",
        "# print(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "# cv = CountVectorizer()   \n",
        "# cv_fit = cv.fit_transform(frequency)    \n",
        "# word_list = cv.get_feature_names() \n",
        "# count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
        "\n",
        "# print(dict(zip(word_list, count_list)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                         #PART 1\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "#Step 2: Import the dataset witht the 20 NewsGroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "#Step 2: Take the Imported Newsgroups and transform the data into a dataframe.\n",
        "print(newsgroups_train.keys())\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "FEATURES, TARGET\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "#Step 3: Perform Exploratory Data Analyis on the data\n",
        "df.head()\n",
        "df.isnull().sum()\n",
        "\n",
        "#Step 4: Isolate the three needed subsets of the 20 NewsGroups, list them, & target the NewsGroups.\n",
        "groups.target_names.index = ['talk religion.misc', 'sci.electronics', 'rec.motorcycles']\n",
        "target_name_index = [groups.target_names.index('talk religion.misc'), groups.target_names.index('sci.electronics'), groups.target_names.index('rec.motorcycles') ]\n",
        "word_index = np.where(groups_target = target_name_index[0])[0]\n",
        "speak_index = np.where(groups_target = target_name_index[1])[0]\n",
        "refrence_index = np.where(groups_target = target_name_index [2])[0]\n",
        "target_index = np.append(np.append(word_index, speak_index), refrence_index)\n",
        "target_documents = [groups[x] for x in target_name_index]\n",
        "\n",
        "#Step 5: Display the top 100 Words\n",
        "counter = CountVectorizer(stop_words='english', max_features=100)\n",
        "words_bag = counter.fit_transform(target_index)\n",
        "sum_of_words = words_bag.sum\n",
        "word_frequency = [(word, sum_of_words[0, index])for word, index in cv.vocabulary_.items()]\n",
        "print('Top 100 words across documents in electronics, religion, and motorcycles')\n",
        "print('------------------------------------------------------------------------')\n",
        "for word, count in word_frequency:\n",
        "  print(word + ':', count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "2ZsDoqZy4viG",
        "outputId": "cbbc92d2-dd6f-48b6-8713-13495aa5a836"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "11314\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aa7c545a17d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#Step 4: Isolate the three needed subsets of the 20 NewsGroups, list them, & target the NewsGroups.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'talk religion.misc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sci.electronics'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rec.motorcycles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mtarget_name_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk religion.misc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sci.electronics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rec.motorcycles'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_name_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'groups' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                 #PART 2\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Step 2: Test if a word would count as a token\n",
        "def letters_only(astr):\n",
        "  return astr.isalpha\n",
        "\n",
        "#Step 3: Strip these words out of the corpus for the given topics and apply lemmatization. \n",
        "stripped_words = ['organization', 'article', 'mr', 'know', 'like', 'com', 'edu', 'lines', 'subject']\n",
        "clean = []\n",
        "lematizer = WordNetLemmatizer()\n",
        "print('Cleaning the list')\n",
        "for post in target_documents:\n",
        "  clean.append(''. join(lemmatizer.lemmatize(word.lower()) for word in post.split() if letters_only(word)and word.lower() not in banned_words))\n",
        "cleaned_words_bag = cv.fit_transform(clean)\n",
        "\n",
        "#Step #4: Find the optimal K\n",
        "print(cv.get_feature_names)\n",
        "print('Finding the optimum K')\n",
        "Sum_of_squared_distance = []\n",
        "K = range(1,15)\n",
        "for k in K\n",
        "  k = KMeans(n_clusters = k)\n",
        "  km = "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "HUIvo6Y-Bccy",
        "outputId": "d7ba7a09-e407-4fbf-ca12-a27f08294df3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-69297ddbb0a7>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    clean.append:\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}