{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapstoneProjectGitHubFinish.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "                                               #PART 1: Display 100 Top Words in Subset of 20 News Groups Data set\n",
        "\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "#Step 2: Import the dataset witht the 20 NewsGroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups()\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "#Step 3: Take the Imported Newsgroups and transform the data into a dataframe.\n",
        "print(newsgroups_train.keys())\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "FEATURES, TARGET\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "#Step 4: Perform Exploratory Data Analyis on the data\n",
        "df.head()\n",
        "df.isnull().sum()\n",
        "\n",
        "#Step 5: Isolate the three needed subsets of the 20 NewsGroups, list them, & target the NewsGroups.\n",
        "target_name_index = [newsgroups_train.target_names.index('talk.religion.misc'),newsgroups_train.target_names.index('sci.electronics'), newsgroups_train.target_names.index('rec.motorcycles') ]\n",
        "word_index = np.where(newsgroups_train.target == target_name_index[0])[0]\n",
        "speak_index = np.where(newsgroups_train.target == target_name_index[1])[0]\n",
        "refrence_index = np.where(newsgroups_train.target == target_name_index [2])[0]\n",
        "target_index = np.append(np.append(word_index, speak_index), refrence_index)\n",
        "target_documents = [newsgroups_train.data[x] for x in target_index]\n",
        "\n",
        "#Step 6: Display the top 100 Words\n",
        "cv = CountVectorizer(stop_words='english', max_features=100)\n",
        "words_bag = cv.fit_transform(target_documents)\n",
        "sum_of_words = words_bag.sum(axis=0)\n",
        "word_frequency = [(word, sum_of_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n",
        "print('Top 100 words across documents in electronics, religion, and motorcycles')\n",
        "print('------------------------------------------------------------------------')\n",
        "for word, count in word_frequency:\n",
        "  print(word + ':', count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZsDoqZy4viG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import EndOfBlock\n",
        "                                                #PART 2: Cluster Documents (Unsupervised Learning) And Discover Topics\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "#Step 2: Test if a word would count as a token\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "def letters_only(astr):\n",
        "  return astr.isalpha()\n",
        "\n",
        "\n",
        "\n",
        "#Step 3: Strip these words out of the corpus for the given topics and apply lemmatization. \n",
        "stripped_words = ['organization', 'article', 'mr', 'know', 'like', 'com', 'edu', 'lines', 'subject', 'university', 'say', 'think']\n",
        "clean = []\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print('Cleaning the list')\n",
        "for post in target_documents:\n",
        "  clean.append(\" \". join(lemmatizer.lemmatize(word.lower()) for word in post.split() if letters_only(word) and word.lower() not in stripped_words))\n",
        "cleaned_words_bag = cv.fit_transform(clean)\n",
        "print(cv.get_feature_names)\n",
        "\n",
        "\n",
        "#Step #4: Find the optimal K\n",
        "print('Finding the optimal K')\n",
        "Sum_of_squared_distance = []\n",
        "K = range(1,16)\n",
        "for k in K:\n",
        "  km = KMeans(n_clusters = k)\n",
        "  km = km.fit(cleaned_words_bag)  \n",
        "  km = Sum_of_squared_distance.append(km.inertia_)\n",
        "\n",
        "#Step 5: Plotting the K Cluster\n",
        "plt.plot(K, Sum_of_squared_distance)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel(Sum_of_squared_distance)\n",
        "plt.title('Elbow Method for the optimal K')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HUIvo6Y-Bccy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Perform Topic Modeling (Must be ran seperatly from the K-Means Cluster)\n",
        "K = 7\n",
        "km = KMeans(n_clusters = K)\n",
        "km = km.fit(cleaned_words_bag)\n",
        "print('iter',km.n_iter_) #<----- Prining the number of iterations\n",
        "print('features',km.n_features_in_) #<-------- Prining the number of features seen duirng the fit\n",
        "\n",
        "\n",
        "for t in range (K-1):\n",
        "   group_indices = np.where(km.labels_ == t )  \n",
        "   group_docs = [clean[x] for x in group_indices[0]]\n",
        "   if len(group_indices[0]) > 2:\n",
        "     fits = cv.fit_transform(group_docs)\n",
        "\n",
        "     print('Group' + str((t + 1))+':')\n",
        "     nmf = NMF(n_components=3, random_state=50).fit(fits)\n",
        "     for topic_idx, topic in enumerate(nmf.components_):\n",
        "       print(topic_idx, ':', ' '.join([cv.get_feature_names()[x] for x in topic.argsort()[:-9:-1]]))\n",
        "       continue\n"
      ],
      "metadata": {
        "id": "ZRLCXT8vVeFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}