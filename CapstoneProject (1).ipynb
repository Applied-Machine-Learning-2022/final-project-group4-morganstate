{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapstoneProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxZaDpExfg2k"
      },
      "outputs": [],
      "source": [
        "#IGNORE THIS CODE, THE CODE FOR PART 1 & 2 IS AFTER THIS CELL\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "#pprint(list(newsgroups_train.target_names))\n",
        "\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "# newsgroups_train['data']\n",
        "\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "# # newsgroups_train.sample(10)\n",
        "\n",
        "\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# #df1 = pd.DataFrame(columns=['sci.electronics', 'talk.religion.misc', 'rec.motorcycles'])\n",
        "# #df1.info()\n",
        "\n",
        "# frequency = ['sci.electronics', 'talk.religion.misc', 'rec.motorcycles']\n",
        "# newsgroups_train  = fetch_20newsgroups(subset='train', categories=frequency)\n",
        "# list(newsgroups_train.target_names)\n",
        "\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "# vectors.shape\n",
        "#vectors.nnz / float(vectors.shape[0])\n",
        "\n",
        "# data = frequency\n",
        "# vectorizer = CountVectorizer()\n",
        "# vectorizer.fit(frequency)\n",
        "\n",
        "# data_vec = vectorizer.transform(frequency)\n",
        "\n",
        "# print(data_vec)\n",
        "# print(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "# cv = CountVectorizer()   \n",
        "# cv_fit = cv.fit_transform(frequency)    \n",
        "# word_list = cv.get_feature_names() \n",
        "# count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
        "\n",
        "# print(dict(zip(word_list, count_list)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                               #PART 1: Display 100 Top Words in Subset of 20 News Groups Data set\n",
        "\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "#Step 2: Import the dataset witht the 20 NewsGroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups()\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "'''\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "#Step 2: Take the Imported Newsgroups and transform the data into a dataframe.\n",
        "print(newsgroups_train.keys())\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "FEATURES, TARGET\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "#Step 3: Perform Exploratory Data Analyis on the data\n",
        "df.head()\n",
        "df.isnull().sum()'''\n",
        "\n",
        "#Step 4: Isolate the three needed subsets of the 20 NewsGroups, list them, & target the NewsGroups.\n",
        "#group.target_names.index = ['talk religion.misc', 'sci.electronics', 'rec.motorcycles']\n",
        "group = ('talk religion.misc', 'sci.electronics', 'rec.motorcycles' )\n",
        "target_name_index = [newsgroups_train.target_names.index('talk.religion.misc'),newsgroups_train.target_names.index('sci.electronics'), newsgroups_train.target_names.index('rec.motorcycles') ]\n",
        "word_index = np.where(newsgroups_train.target == target_name_index[0])[0]\n",
        "speak_index = np.where(newsgroups_train.target == target_name_index[1])[0]\n",
        "refrence_index = np.where(newsgroups_train.target== target_name_index [2])[0]\n",
        "target_index = np.append(np.append(word_index, speak_index), refrence_index)\n",
        "target_documents = [newsgroups_train.data[x] for x in target_name_index]\n",
        "\n",
        "#Step 5: Display the top 100 Words\n",
        "cv = CountVectorizer(stop_words='english', max_features=100)\n",
        "words_bag = cv.fit_transform(target_documents)\n",
        "sum_of_words = words_bag.sum(axis=0)\n",
        "word_frequency = [(word, sum_of_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n",
        "print('Top 100 words across documents in electronics, religion, and motorcycles')\n",
        "print('------------------------------------------------------------------------')\n",
        "for word, count in word_frequency:\n",
        "  print(word + ':', count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZsDoqZy4viG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c60981-3c8e-4f0d-9d80-f8c40b84ffd2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "Top 100 words across documents in electronics, religion, and motorcycles\n",
            "------------------------------------------------------------------------\n",
            "terminal: 4\n",
            "ncd: 4\n",
            "subject: 3\n",
            "file: 3\n",
            "organization: 3\n",
            "lines: 3\n",
            "boots: 3\n",
            "tcp: 3\n",
            "ip: 3\n",
            "access: 3\n",
            "control: 3\n",
            "list: 3\n",
            "3b2: 3\n",
            "help: 3\n",
            "fc: 3\n",
            "hp: 3\n",
            "com: 3\n",
            "rod: 3\n",
            "cerkoney: 3\n",
            "abarden: 2\n",
            "ann: 2\n",
            "marie: 2\n",
            "barden: 2\n",
            "question: 2\n",
            "entry: 2\n",
            "parameter: 2\n",
            "syntax: 2\n",
            "configuration: 2\n",
            "loaded: 2\n",
            "add: 2\n",
            "x11r3: 2\n",
            "sun: 2\n",
            "edit: 2\n",
            "appreciated: 2\n",
            "rodc: 2\n",
            "hewlett: 2\n",
            "packard: 2\n",
            "fort: 2\n",
            "collins: 2\n",
            "win: 2\n",
            "icons: 2\n",
            "tybse1: 1\n",
            "uucp: 1\n",
            "config: 1\n",
            "tybrin: 1\n",
            "corporation: 1\n",
            "shalimar: 1\n",
            "distribution: 1\n",
            "usa: 1\n",
            "unix: 1\n",
            "systems: 1\n",
            "running: 1\n",
            "ss10: 1\n",
            "want: 1\n",
            "window: 1\n",
            "time: 1\n",
            "set: 1\n",
            "telnet: 1\n",
            "session: 1\n",
            "ve: 1\n",
            "tried: 1\n",
            "work: 1\n",
            "wrong: 1\n",
            "bogus: 1\n",
            "trying: 1\n",
            "containing: 1\n",
            "worthless: 1\n",
            "thanks: 1\n",
            "afseo: 1\n",
            "eglin: 1\n",
            "af: 1\n",
            "nntp: 1\n",
            "posting: 1\n",
            "newsreader: 1\n",
            "tin: 1\n",
            "version: 1\n",
            "pl8: 1\n",
            "regards: 1\n",
            "______________________________________________: 1\n",
            "ms: 1\n",
            "37: 1\n",
            "east: 1\n",
            "rd: 1\n",
            "80525: 1\n",
            "ux: 1\n",
            "_____________________________________________: 1\n",
            "__: 1\n",
            "uni: 1\n",
            "edu: 1\n",
            "university: 1\n",
            "northern: 1\n",
            "10: 1\n",
            "downloaded: 1\n",
            "bmp: 1\n",
            "change: 1\n",
            "wallpaper: 1\n",
            "use: 1\n",
            "thanx: 1\n",
            "brando: 1\n",
            "ps: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                #PART 2: Cluster Documents (Unsupervised Learning) And Discover Topics\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Step 2: Test if a word would count as a token\n",
        "def letters_only(astr):\n",
        "  return astr.isalpha\n",
        "\n",
        "#Step 3: Strip these words out of the corpus for the given topics and apply lemmatization. \n",
        "stripped_words = ['organization', 'article', 'mr', 'know', 'like', 'com', 'edu', 'lines', 'subject']\n",
        "clean = []\n",
        "lematizer = WordNetLemmatizer()\n",
        "print('Cleaning the list')\n",
        "for post in target_documents:\n",
        "  clean.append(''. join(lemmatizer.lemmatize(word.lower()) for word in post.split() if letters_only(word)and word.lower() not in stripped_words))\n",
        "cleaned_words_bag = cv.fit_transform(clean)\n",
        "\n",
        "#Step #4: Find the optimal K\n",
        "print(cv.get_feature_names)\n",
        "print('Finding the optimum K')\n",
        "Sum_of_squared_distance = []\n",
        "K = 8\n",
        "\n",
        "km = KMeans(n_clusters = k)\n",
        "km = km.fit(cleaned_words_bag)\n",
        "Sum_of_squared_distance.append(km.inertia)\n",
        "\n",
        "plt.plot(k, Sum_of_squared_distance)\n",
        "plt.ylabel(Sum_of_squared_distance) \n",
        "plt.xlabel('k')\n",
        "plt.title('The Optimal K')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HUIvo6Y-Bccy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "a53f3118-41df-43e6-d1c3-df69b2e42565"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning the list\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-a61e34f7e5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cleaning the list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mletters_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstripped_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mcleaned_words_bag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-a61e34f7e5bc>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cleaning the list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mletters_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstripped_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mcleaned_words_bag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lemmatizer' is not defined"
          ]
        }
      ]
    }
  ]
}