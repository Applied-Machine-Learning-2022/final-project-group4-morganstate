{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapstoneProjectFinish.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "XxZaDpExfg2k",
        "outputId": "ed1fa9c3-7ed6-42ad-adfd-1a6c0eca2622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "11314\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       0  titles\n",
              "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
              "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4\n",
              "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4\n",
              "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1\n",
              "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14\n",
              "...                                                  ...     ...\n",
              "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...      13\n",
              "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...       4\n",
              "11311  From: westes@netcom.com (Will Estes)\\nSubject:...       3\n",
              "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...       1\n",
              "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...       8\n",
              "\n",
              "[11314 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93ffeb78-e6d5-410b-999d-8838e1804a84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>titles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11310</th>\n",
              "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11311</th>\n",
              "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11312</th>\n",
              "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11313</th>\n",
              "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11314 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93ffeb78-e6d5-410b-999d-8838e1804a84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93ffeb78-e6d5-410b-999d-8838e1804a84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93ffeb78-e6d5-410b-999d-8838e1804a84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#IGNORE THIS CODE, THE CODE FOR PART 1 & 2 IS AFTER THIS CELL\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "#pprint(list(newsgroups_train.target_names))\n",
        "\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "# newsgroups_train['data']\n",
        "\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "# # newsgroups_train.sample(10)\n",
        "\n",
        "\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# #df1 = pd.DataFrame(columns=['sci.electronics', 'talk.religion.misc', 'rec.motorcycles'])\n",
        "# #df1.info()\n",
        "\n",
        "# frequency = ['sci.electronics', 'talk.religion.misc', 'rec.motorcycles']\n",
        "# newsgroups_train  = fetch_20newsgroups(subset='train', categories=frequency)\n",
        "# list(newsgroups_train.target_names)\n",
        "\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "# vectors.shape\n",
        "#vectors.nnz / float(vectors.shape[0])\n",
        "\n",
        "# data = frequency\n",
        "# vectorizer = CountVectorizer()\n",
        "# vectorizer.fit(frequency)\n",
        "\n",
        "# data_vec = vectorizer.transform(frequency)\n",
        "\n",
        "# print(data_vec)\n",
        "# print(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "# cv = CountVectorizer()   \n",
        "# cv_fit = cv.fit_transform(frequency)    \n",
        "# word_list = cv.get_feature_names() \n",
        "# count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
        "\n",
        "# print(dict(zip(word_list, count_list)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                               #PART 1: Display 100 Top Words in Subset of 20 News Groups Data set\n",
        "\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "#Step 2: Import the dataset witht the 20 NewsGroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups()\n",
        "print(newsgroups_train.keys())\n",
        "\n",
        "\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "\n",
        "FEATURES, TARGET\n",
        "\n",
        "#Step 3: Take the Imported Newsgroups and transform the data into a dataframe.\n",
        "print(newsgroups_train.keys())\n",
        "FEATURES = newsgroups_train['filenames']\n",
        "TARGET = 'titles'\n",
        "FEATURES, TARGET\n",
        "print(len(newsgroups_train['data']))\n",
        "df = pd.DataFrame(newsgroups_train['data'])\n",
        "df\n",
        "df[TARGET] = newsgroups_train['target']\n",
        "df\n",
        "\n",
        "#Step 4: Perform Exploratory Data Analyis on the data\n",
        "df.head()\n",
        "df.isnull().sum()\n",
        "\n",
        "#Step 3: Isolate the three needed subsets of the 20 NewsGroups, list them, & target the NewsGroups.\n",
        "target_name_index = [newsgroups_train.target_names.index('talk.religion.misc'),newsgroups_train.target_names.index('sci.electronics'), newsgroups_train.target_names.index('rec.motorcycles') ]\n",
        "word_index = np.where(newsgroups_train.target == target_name_index[0])[0]\n",
        "speak_index = np.where(newsgroups_train.target == target_name_index[1])[0]\n",
        "refrence_index = np.where(newsgroups_train.target == target_name_index [2])[0]\n",
        "target_index = np.append(np.append(word_index, speak_index), refrence_index)\n",
        "target_documents = [newsgroups_train.data[x] for x in target_index]\n",
        "\n",
        "#Step 4: Display the top 100 Words\n",
        "cv = CountVectorizer(stop_words='english', max_features=100)\n",
        "words_bag = cv.fit_transform(target_documents)\n",
        "sum_of_words = words_bag.sum(axis=0)\n",
        "word_frequency = [(word, sum_of_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "word_frequency = sorted(word_frequency, key = lambda x: x[1], reverse=True)\n",
        "print('Top 100 words across documents in electronics, religion, and motorcycles')\n",
        "print('------------------------------------------------------------------------')\n",
        "for word, count in word_frequency:\n",
        "  print(word + ':', count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZsDoqZy4viG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8048525-6a3e-41a6-8a38-b8537b1463be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "11314\n",
            "Top 100 words across documents in electronics, religion, and motorcycles\n",
            "------------------------------------------------------------------------\n",
            "edu: 2349\n",
            "com: 2083\n",
            "subject: 1705\n",
            "lines: 1647\n",
            "organization: 1596\n",
            "writes: 1162\n",
            "article: 1115\n",
            "like: 811\n",
            "don: 787\n",
            "posting: 757\n",
            "just: 754\n",
            "host: 725\n",
            "know: 706\n",
            "nntp: 698\n",
            "good: 616\n",
            "people: 610\n",
            "university: 604\n",
            "ca: 601\n",
            "bike: 548\n",
            "does: 541\n",
            "god: 537\n",
            "use: 522\n",
            "dod: 474\n",
            "think: 456\n",
            "time: 456\n",
            "ve: 430\n",
            "way: 405\n",
            "new: 403\n",
            "want: 392\n",
            "jesus: 391\n",
            "used: 374\n",
            "say: 371\n",
            "world: 367\n",
            "distribution: 357\n",
            "make: 352\n",
            "reply: 341\n",
            "need: 339\n",
            "really: 332\n",
            "work: 322\n",
            "right: 316\n",
            "power: 308\n",
            "did: 295\n",
            "john: 287\n",
            "christian: 286\n",
            "question: 279\n",
            "ll: 279\n",
            "ground: 268\n",
            "uk: 263\n",
            "org: 263\n",
            "thing: 261\n",
            "going: 257\n",
            "got: 255\n",
            "sun: 253\n",
            "point: 246\n",
            "said: 243\n",
            "help: 243\n",
            "cs: 242\n",
            "hp: 242\n",
            "10: 240\n",
            "david: 239\n",
            "little: 234\n",
            "usa: 234\n",
            "life: 232\n",
            "state: 225\n",
            "long: 219\n",
            "years: 218\n",
            "ac: 218\n",
            "ride: 216\n",
            "look: 215\n",
            "line: 213\n",
            "cc: 213\n",
            "thanks: 212\n",
            "bible: 210\n",
            "computer: 210\n",
            "mail: 209\n",
            "things: 209\n",
            "read: 209\n",
            "sure: 208\n",
            "believe: 206\n",
            "current: 205\n",
            "better: 204\n",
            "old: 202\n",
            "wire: 200\n",
            "doesn: 198\n",
            "sandvik: 195\n",
            "law: 195\n",
            "high: 192\n",
            "says: 191\n",
            "post: 190\n",
            "circuit: 189\n",
            "case: 188\n",
            "wrote: 187\n",
            "day: 185\n",
            "probably: 184\n",
            "number: 182\n",
            "problem: 181\n",
            "20: 179\n",
            "actually: 178\n",
            "different: 177\n",
            "15: 177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import EndOfBlock\n",
        "                                                #PART 2: Cluster Documents (Unsupervised Learning) And Discover Topics\n",
        "#Step 1 Import all the functions needed\n",
        "from operator import index\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "#Step 2: Test if a word would count as a token\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "def letters_only(astr):\n",
        "  return astr.isalpha()\n",
        "\n",
        "\n",
        "\n",
        "#Step 3: Strip these words out of the corpus for the given topics and apply lemmatization. \n",
        "stripped_words = ['organization', 'article', 'mr', 'know', 'like', 'com', 'edu', 'lines', 'subject', 'university', 'say', 'think']\n",
        "clean = []\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print('Cleaning the list')\n",
        "for post in target_documents:\n",
        "  clean.append(\" \". join(lemmatizer.lemmatize(word.lower()) for word in post.split() if letters_only(word) and word.lower() not in stripped_words))\n",
        "cleaned_words_bag = cv.fit_transform(clean)\n",
        "print(cv.get_feature_names)\n",
        "\n",
        "\n",
        "#Step #4: Find the optimal K\n",
        "print('Finding the optimal K')\n",
        "Sum_of_squared_distance = []\n",
        "K = range(1,16)\n",
        "for k in K:\n",
        "  km = KMeans(n_clusters = k)\n",
        "  km = km.fit(cleaned_words_bag)  \n",
        "  km = Sum_of_squared_distance.append(km.inertia_)\n",
        "\n",
        "#Step 5: Plotting the K Cluster\n",
        "plt.plot(K, Sum_of_squared_distance)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel(Sum_of_squared_distance)\n",
        "plt.title('Elbow Method for the optimal K')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HUIvo6Y-Bccy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bzLmoT1aj2r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "K = 7\n",
        "km = KMeans(n_clusters = K)\n",
        "km = km.fit(cleaned_words_bag)\n",
        "print('iter',km.n_iter_) #<----- Prining the number of iterations\n",
        "print('features',km.n_features_in_) #<-------- Prining the number of features seen duirng the fit\n",
        "\n",
        "\n",
        "for t in range (K-1):\n",
        "  group_indices = np.where(km.labels_ == t )  \n",
        "  group_docs = [clean[x] for x in group_indices[0]]\n",
        "  if len(group_indices[0]) > 2:\n",
        "    fits = cv.fit_transform(group_docs)\n",
        "\n",
        "    print('Group' + str((t + 1))+':')\n",
        "    nmf = NMF(n_components=3, random_state=50).fit(fits)\n",
        "    for topic_idx, topic in enumerate(nmf.components_):\n",
        "      print(topic_idx, ':', ' '.join([cv.get_feature_names()[x] for x in topic.argsort()[:-9:-1]]))\n",
        "      continue\n",
        "\n",
        "km = km.fit(cleaned_words_bag)"
      ],
      "metadata": {
        "id": "LdvGzinvYjNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}